{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [rouge](https://pypi.org/project/rouge/)\n",
    "- Note: \"f\" stands for f1_score, \"p\" stands for precision, \"r\" stands for recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.5528035775713794,\n",
       "  'p': 0.8539562289562289,\n",
       "  'f': 0.6512670259900423},\n",
       " 'rouge-2': {'r': 0.3353174603174603,\n",
       "  'p': 0.5244559362206421,\n",
       "  'f': 0.3928074411537155},\n",
       " 'rouge-l': {'r': 0.5369305616983636,\n",
       "  'p': 0.8122895622895623,\n",
       "  'f': 0.6282785202429159}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out = [\"he began by starting a five person war cabinet and included chamberlain as lord president of the council\",\n",
    "             \"the siege lasted from 250 to 241 bc, the romans laid siege to lilybaeum\",\n",
    "             \"the original ocean water was found in aquaculture\"]\n",
    "\n",
    "reference = [\"he began his premiership by forming a five-man war cabinet which included chamberlain as lord president of the council\",\n",
    "             \"the siege of lilybaeum lasted from 250 to 241 bc, as the roman army laid siege to the carthaginian-held sicilian city of lilybaeum\",\n",
    "             \"the original mission was for research into the uses of deep ocean water in ocean thermal energy conversion (otec) renewable energy production and in aquaculture\"]\n",
    "rouge = Rouge()\n",
    "rouge.get_scores(model_out, reference, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghae/anaconda3/envs/persona_extraction/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = [[\"this\", \"is\", \"the\", \"good\", \"choice\"], \n",
    "\t     [\"it\", \"is\", \"a\", \"sample\"]]\n",
    "candidate = ['this', \"is\", \"the\", \"sample\"]\n",
    "score2 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Sentence :  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "reference = [[[\"this\", \"is\", \"a\", \"sample\"]], \n",
    " \t     [[\"another\", \"sentence\"], [\"other\", \"sentence\"]]]\n",
    "candidate = [['this', \"is\", \"a\", \"sample\"], \n",
    "\t     ['just', \"another\", \"sentence\"]]\n",
    "score4 = corpus_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "\n",
    "print(\"Multiple Sentence : \", score4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나는 인천 남자이다 나는 인천에 산다',\n",
       " '나는 요즘 학교에서 동아리 활동을 하고 있다',\n",
       " '나는 아직 대학 가기 전이라서 놀고 있다',\n",
       " '나는 부모님 따라 국내여행만 다녔다',\n",
       " '나는 부모님과 여행 가는 걸 좋아한다',\n",
       " '나는 게임이나 음악 듣는 걸 좋아한다',\n",
       " '나는 양식이랑 한식을 좋아하고 중식은 속이 더부룩해서 싫다.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = '<s> 나는 인천 남자이다 나는 인천에 산다,나는 요즘 학교에서 동아리 활동을 하고 있다,나는 아직 대학 가기 전이라서 놀고 있다,나는 부모님 따라 국내여행만 다녔다,나는 부모님과 여행 가는 걸 좋아한다,나는 게임이나 음악 듣는 걸 좋아한다,나는 양식이랑 한식을 좋아하고 중식은 속이 더부룩해서 싫다. </s>'.replace('<s>','').replace('</s>','').strip().split(',')\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = ['나는 10대 남성이다 나의 거주지는 인천이다','나는 동아리 활동을 하고 있다','나는 동아리 활동이 즐겁다','나는 부모님과 국내 여행을 다닌다','나는 사람들과 잘 어울린다','나는 게임과 음악을 즐긴다','나는 양식, 한식을 좋아한다 중식을 싫어한다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나는 10대 남성이다 나의 거주지는 인천이다',\n",
       " '나는 동아리 활동을 하고 있다',\n",
       " '나는 동아리 활동이 즐겁다',\n",
       " '나는 부모님과 국내 여행을 다닌다',\n",
       " '나는 사람들과 잘 어울린다',\n",
       " '나는 게임과 음악을 즐긴다',\n",
       " '나는 양식, 한식을 좋아한다 중식을 싫어한다.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.35000000000000003,\n",
       "  'p': 0.26292517006802724,\n",
       "  'f': 0.29752627839116236},\n",
       " 'rouge-2': {'r': 0.10714285714285714,\n",
       "  'p': 0.07142857142857142,\n",
       "  'f': 0.08571428502857144},\n",
       " 'rouge-l': {'r': 0.35000000000000003,\n",
       "  'p': 0.26292517006802724,\n",
       "  'f': 0.29752627839116236}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "rouge.get_scores(filtered, reference, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rouge.get_scores(filtered, reference, avg=True)['rouge-l']['r'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3987341772151899"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu(reference, filtered, weights=(1, 0, 0, 0)) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.736"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu(reference, reference, weights=(1, 0, 0, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona_extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
